{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc2576ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df41678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./train.csv')\n",
    "df_test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cc3a79",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "997aefcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Both datasets have the same columns (excluding target)\n",
      "\n",
      "Comparing data types for 25 common columns:\n",
      "------------------------------------------------------------\n",
      "✓ age                                 | Train: int64      | Test: int64\n",
      "✓ alcohol_consumption_per_week        | Train: int64      | Test: int64\n",
      "✓ bmi                                 | Train: float64    | Test: float64\n",
      "✓ cardiovascular_history              | Train: int64      | Test: int64\n",
      "✓ cholesterol_total                   | Train: int64      | Test: int64\n",
      "✓ diastolic_bp                        | Train: int64      | Test: int64\n",
      "✓ diet_score                          | Train: float64    | Test: float64\n",
      "✓ education_level                     | Train: object     | Test: object\n",
      "✓ employment_status                   | Train: object     | Test: object\n",
      "✓ ethnicity                           | Train: object     | Test: object\n",
      "✓ family_history_diabetes             | Train: int64      | Test: int64\n",
      "✓ gender                              | Train: object     | Test: object\n",
      "✓ hdl_cholesterol                     | Train: int64      | Test: int64\n",
      "✓ heart_rate                          | Train: int64      | Test: int64\n",
      "✓ hypertension_history                | Train: int64      | Test: int64\n",
      "✓ id                                  | Train: int64      | Test: int64\n",
      "✓ income_level                        | Train: object     | Test: object\n",
      "✓ ldl_cholesterol                     | Train: int64      | Test: int64\n",
      "✓ physical_activity_minutes_per_week  | Train: int64      | Test: int64\n",
      "✓ screen_time_hours_per_day           | Train: float64    | Test: float64\n",
      "✓ sleep_hours_per_day                 | Train: float64    | Test: float64\n",
      "✓ smoking_status                      | Train: object     | Test: object\n",
      "✓ systolic_bp                         | Train: int64      | Test: int64\n",
      "✓ triglycerides                       | Train: int64      | Test: int64\n",
      "✓ waist_to_hip_ratio                  | Train: float64    | Test: float64\n"
     ]
    }
   ],
   "source": [
    "# Get common columns\n",
    "common_cols = set(df_train.columns) & set(df_test.columns)\n",
    "\n",
    "# Check if they have the same columns (excluding target column)\n",
    "train_cols = set(df_train.columns) - {'diagnosed_diabetes'}\n",
    "test_cols = set(df_test.columns)\n",
    "\n",
    "if train_cols == test_cols:\n",
    "    print(\"✓ Both datasets have the same columns (excluding target)\")\n",
    "else:\n",
    "    print(\"✗ Datasets do NOT have the same columns\")\n",
    "\n",
    "print(f\"\\nComparing data types for {len(common_cols)} common columns:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for col in sorted(common_cols):\n",
    "    train_dtype = df_train[col].dtype\n",
    "    test_dtype = df_test[col].dtype\n",
    "    \n",
    "    if train_dtype == test_dtype:\n",
    "        status = \"✓\"\n",
    "    else:\n",
    "        status = \"✗\"\n",
    "    \n",
    "    print(f\"{status} {col:<35} | Train: {str(train_dtype):<10} | Test: {str(test_dtype)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a4460a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ No null values found in train dataset\n"
     ]
    }
   ],
   "source": [
    "null_counts_train = df_train.isnull().sum()\n",
    "total_nulls_train = null_counts_train.sum()\n",
    "\n",
    "for col in df_train.columns:\n",
    "    null_count = null_counts_train[col]\n",
    "    if null_count > 0:\n",
    "        print(f\"{col:<35} | {null_count:>5} nulls\")\n",
    "\n",
    "if total_nulls_train == 0:\n",
    "    print(\"✓ No null values found in train dataset\")\n",
    "else:\n",
    "    print(f\"\\nTotal null values in train: {total_nulls_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b252a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features (18):\n",
      "                                      mean    std     min     max\n",
      "age                                  50.36  11.66   19.00   89.00\n",
      "alcohol_consumption_per_week          2.07   1.05    1.00    9.00\n",
      "physical_activity_minutes_per_week   80.23  51.20    1.00  747.00\n",
      "diet_score                            5.96   1.46    0.10    9.90\n",
      "sleep_hours_per_day                   7.00   0.90    3.10    9.90\n",
      "screen_time_hours_per_day             6.01   2.02    0.60   16.50\n",
      "bmi                                  25.87   2.86   15.10   38.40\n",
      "waist_to_hip_ratio                    0.86   0.04    0.68    1.05\n",
      "systolic_bp                         116.29  11.01   91.00  163.00\n",
      "diastolic_bp                         75.44   6.83   51.00  104.00\n",
      "heart_rate                           70.17   6.94   42.00  101.00\n",
      "cholesterol_total                   186.82  16.73  117.00  289.00\n",
      "hdl_cholesterol                      53.82   8.27   21.00   90.00\n",
      "ldl_cholesterol                     102.91  19.02   51.00  205.00\n",
      "triglycerides                       123.08  24.74   31.00  290.00\n",
      "family_history_diabetes               0.15   0.36    0.00    1.00\n",
      "hypertension_history                  0.18   0.39    0.00    1.00\n",
      "cardiovascular_history                0.03   0.17    0.00    1.00\n"
     ]
    }
   ],
   "source": [
    "# Numerical features summary (limited output)\n",
    "numerical_cols = df_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_cols = [col for col in numerical_cols if col not in ['id', 'diagnosed_diabetes']]\n",
    "\n",
    "print(f\"Numerical features ({len(numerical_cols)}):\")\n",
    "print(df_train[numerical_cols].describe().T[['mean', 'std', 'min', 'max']].round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "09fb7196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features (6):\n",
      "  gender: 3 unique values\n",
      "  ethnicity: 5 unique values\n",
      "  education_level: 4 unique values\n",
      "  income_level: 5 unique values\n",
      "  smoking_status: 3 unique values\n",
      "  employment_status: 4 unique values\n"
     ]
    }
   ],
   "source": [
    "# Categorical features\n",
    "categorical_cols = df_train.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Categorical features ({len(categorical_cols)}):\")\n",
    "for col in categorical_cols:\n",
    "    unique_count = df_train[col].nunique()\n",
    "    print(f\"  {col}: {unique_count} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a2db677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top correlations with target:\n",
      "diagnosed_diabetes                    1.000\n",
      "family_history_diabetes               0.211\n",
      "physical_activity_minutes_per_week    0.170\n",
      "age                                   0.161\n",
      "systolic_bp                           0.107\n",
      "bmi                                   0.106\n",
      "ldl_cholesterol                       0.103\n",
      "triglycerides                         0.091\n",
      "cholesterol_total                     0.088\n",
      "waist_to_hip_ratio                    0.081\n",
      "hdl_cholesterol                       0.053\n",
      "Name: diagnosed_diabetes, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Correlation with target (top 10)\n",
    "correlations = df_train[numerical_cols + ['diagnosed_diabetes']].corr()['diagnosed_diabetes'].abs().sort_values(ascending=False)\n",
    "print(\"Top correlations with target:\")\n",
    "print(correlations.head(11).round(3))  # 11 to exclude target itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f014df73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with outliers (>1.5*IQR):\n",
      "  hypertension_history: 127393 (18.2%)\n",
      "  family_history_diabetes: 104581 (14.9%)\n",
      "  physical_activity_minutes_per_week: 33490 (4.8%)\n",
      "  cardiovascular_history: 21227 (3.0%)\n",
      "  triglycerides: 9053 (1.3%)\n",
      "  waist_to_hip_ratio: 6159 (0.9%)\n",
      "  sleep_hours_per_day: 6152 (0.9%)\n",
      "  diastolic_bp: 5752 (0.8%)\n",
      "  hdl_cholesterol: 4693 (0.7%)\n",
      "  bmi: 4254 (0.6%)\n"
     ]
    }
   ],
   "source": [
    "# Check for outliers (IQR method) - only show features with significant outliers\n",
    "outlier_summary = {}\n",
    "for col in numerical_cols:\n",
    "    Q1 = df_train[col].quantile(0.25)\n",
    "    Q3 = df_train[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = ((df_train[col] < lower_bound) | (df_train[col] > upper_bound)).sum()\n",
    "    if outliers > 0:\n",
    "        outlier_summary[col] = outliers\n",
    "\n",
    "if outlier_summary:\n",
    "    print(\"Features with outliers (>1.5*IQR):\")\n",
    "    for col, count in sorted(outlier_summary.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        pct = (count / len(df_train)) * 100\n",
    "        print(f\"  {col}: {count} ({pct:.1f}%)\")\n",
    "else:\n",
    "    print(\"No significant outliers detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4bc5a6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender\n",
      "Other     0.641\n",
      "Male      0.624\n",
      "Female    0.622\n",
      "Name: diagnosed_diabetes, dtype: float64\n",
      "\n",
      "\n",
      "ethnicity\n",
      "Other       0.636\n",
      "Asian       0.628\n",
      "White       0.624\n",
      "Black       0.624\n",
      "Hispanic    0.616\n",
      "Name: diagnosed_diabetes, dtype: float64\n",
      "\n",
      "\n",
      "education_level\n",
      "No formal       0.636\n",
      "Graduate        0.627\n",
      "Highschool      0.621\n",
      "Postgraduate    0.617\n",
      "Name: diagnosed_diabetes, dtype: float64\n",
      "\n",
      "\n",
      "income_level\n",
      "Low             0.630\n",
      "Lower-Middle    0.627\n",
      "High            0.624\n",
      "Upper-Middle    0.620\n",
      "Middle          0.620\n",
      "Name: diagnosed_diabetes, dtype: float64\n",
      "\n",
      "\n",
      "smoking_status\n",
      "Former     0.625\n",
      "Current    0.623\n",
      "Never      0.623\n",
      "Name: diagnosed_diabetes, dtype: float64\n",
      "\n",
      "\n",
      "employment_status\n",
      "Employed      0.625\n",
      "Unemployed    0.622\n",
      "Student       0.622\n",
      "Retired       0.618\n",
      "Name: diagnosed_diabetes, dtype: float64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Target by categorical features (top categories only)\n",
    "for col in categorical_cols:\n",
    "    target_by_cat = df_train.groupby(col)['diagnosed_diabetes'].mean().sort_values(ascending=False)\n",
    "    print(target_by_cat.head(5).round(3))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f288003",
   "metadata": {},
   "source": [
    "# Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "92968348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Education missing: 0 train, 0 test\n",
      "Income missing: 0 train, 0 test\n"
     ]
    }
   ],
   "source": [
    "# Ordinal encoding for variables with natural order\n",
    "education_order = {'No formal': 0, 'Highschool': 1, 'Graduate': 2, 'Postgraduate': 3}\n",
    "income_order = {'Low': 0, 'Lower-Middle': 1, 'Middle': 2, 'Upper-Middle': 3, 'High': 4}\n",
    "\n",
    "df_train['education_level_enc'] = df_train['education_level'].map(education_order)\n",
    "df_train['income_level_enc'] = df_train['income_level'].map(income_order)\n",
    "\n",
    "df_test['education_level_enc'] = df_test['education_level'].map(education_order)\n",
    "df_test['income_level_enc'] = df_test['income_level'].map(income_order)\n",
    "\n",
    "# Verify mappings worked\n",
    "print(f\"Education missing: {df_train['education_level_enc'].isna().sum()} train, {df_test['education_level_enc'].isna().sum()} test\")\n",
    "print(f\"Income missing: {df_train['income_level_enc'].isna().sum()} train, {df_test['income_level_enc'].isna().sum()} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5c23b35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoding complete for: ['gender', 'ethnicity', 'smoking_status', 'employment_status']\n"
     ]
    }
   ],
   "source": [
    "# Label encoding for nominal categorical variables (for tree-based models)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "nominal_cols = ['gender', 'ethnicity', 'smoking_status', 'employment_status']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in nominal_cols:\n",
    "    le = LabelEncoder()\n",
    "    # Fit on combined data to ensure consistency\n",
    "    le.fit(pd.concat([df_train[col], df_test[col]]))\n",
    "    df_train[f'{col}_enc'] = le.transform(df_train[col])\n",
    "    df_test[f'{col}_enc'] = le.transform(df_test[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(\"Label encoding complete for:\", nominal_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "87706971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 24\n",
      "  Numerical: 18\n",
      "  Categorical (encoded): 6\n"
     ]
    }
   ],
   "source": [
    "# Define feature columns for modeling\n",
    "encoded_cat_cols = [f'{col}_enc' for col in categorical_cols]\n",
    "feature_cols = numerical_cols + encoded_cat_cols\n",
    "\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "print(f\"  Numerical: {len(numerical_cols)}\")\n",
    "print(f\"  Categorical (encoded): {len(encoded_cat_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b6ec729a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot features added: ['gender_Male', 'gender_Other', 'ethnicity_Black', 'ethnicity_Hispanic', 'ethnicity_Other', 'ethnicity_White', 'smoking_status_Former', 'smoking_status_Never', 'employment_status_Retired', 'employment_status_Student', 'employment_status_Unemployed']\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encoding for linear models (Logistic Regression, SVM, etc.)\n",
    "# Only for nominal variables (no natural order)\n",
    "nominal_cols = ['gender', 'ethnicity', 'smoking_status', 'employment_status']\n",
    "\n",
    "# Create one-hot encoded features (drop_first=True to avoid multicollinearity)\n",
    "df_train_ohe = pd.get_dummies(df_train[nominal_cols], drop_first=True)\n",
    "df_test_ohe = pd.get_dummies(df_test[nominal_cols], drop_first=True)\n",
    "\n",
    "# Ensure both have same columns\n",
    "missing_in_test = set(df_train_ohe.columns) - set(df_test_ohe.columns)\n",
    "for col in missing_in_test:\n",
    "    df_test_ohe[col] = 0\n",
    "df_test_ohe = df_test_ohe[df_train_ohe.columns]\n",
    "\n",
    "# Add to dataframes\n",
    "df_train = pd.concat([df_train, df_train_ohe], axis=1)\n",
    "df_test = pd.concat([df_test, df_test_ohe], axis=1)\n",
    "\n",
    "print(f\"One-Hot features added: {list(df_train_ohe.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec1167e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for tree models: 24\n",
      "Features for linear models: 31\n"
     ]
    }
   ],
   "source": [
    "# Feature sets for different model types\n",
    "ohe_cols = list(df_train_ohe.columns)\n",
    "ordinal_cols = ['education_level_enc', 'income_level_enc']\n",
    "\n",
    "# For tree-based models (XGBoost, LightGBM, RandomForest) - can use label encoding\n",
    "features_tree = numerical_cols + encoded_cat_cols\n",
    "\n",
    "# For linear models (Logistic Regression, SVM) - use ordinal + one-hot\n",
    "features_linear = numerical_cols + ordinal_cols + ohe_cols\n",
    "\n",
    "print(f\"Features for tree models: {len(features_tree)}\")\n",
    "print(f\"Features for linear models: {len(features_linear)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a554fa18",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "57366e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 560,000 samples\n",
      "Val: 140,000 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Prepare data\n",
    "X = df_train[features_tree]\n",
    "y = df_train['diagnosed_diabetes']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Val: {X_val.shape[0]:,} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "735dc9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scaled for linear models\n"
     ]
    }
   ],
   "source": [
    "# Prepare scaled data for linear models\n",
    "X_linear = df_train[features_linear]\n",
    "X_train_lin, X_val_lin, _, _ = train_test_split(X_linear, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_lin)\n",
    "X_val_scaled = scaler.transform(X_val_lin)\n",
    "\n",
    "print(\"Data scaled for linear models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "07e0b083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.6644\n",
      "CPU times: total: 1.61 s\n",
      "Wall time: 413 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1. Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "lr_pred = lr.predict(X_val_scaled)\n",
    "lr_acc = accuracy_score(y_val, lr_pred)\n",
    "print(f\"Logistic Regression: {lr_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df614478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: 0.6650\n",
      "CPU times: total: 3min 6s\n",
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 2. Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_val)\n",
    "rf_acc = accuracy_score(y_val, rf_pred)\n",
    "print(f\"Random Forest: {rf_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aef2dbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting: 0.6770\n",
      "CPU times: total: 3min 49s\n",
      "Wall time: 3min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 3. Gradient Boosting\n",
    "gb = GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "gb_pred = gb.predict(X_val)\n",
    "gb_acc = accuracy_score(y_val, gb_pred)\n",
    "print(f\"Gradient Boosting: {gb_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4cea23d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost (GPU): 0.6783\n",
      "CPU times: total: 6.19 s\n",
      "Wall time: 1.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 4. XGBoost (GPU)\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=100, \n",
    "    max_depth=6, \n",
    "    learning_rate=0.1, \n",
    "    random_state=42,\n",
    "    tree_method='hist',\n",
    "    device='cuda',\n",
    "    verbosity=0\n",
    ")\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_pred = xgb.predict(X_val)\n",
    "xgb_acc = accuracy_score(y_val, xgb_pred)\n",
    "print(f\"XGBoost (GPU): {xgb_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1d36679e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM (GPU): 0.6794\n",
      "CPU times: total: 13 s\n",
      "Wall time: 2.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 5. LightGBM (GPU)\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm = LGBMClassifier(\n",
    "    n_estimators=100, \n",
    "    max_depth=6, \n",
    "    learning_rate=0.1, \n",
    "    random_state=42, \n",
    "    device='gpu',  # <-- GPU acceleration\n",
    "    verbose=-1\n",
    ")\n",
    "lgbm.fit(X_train, y_train)\n",
    "lgbm_pred = lgbm.predict(X_val)\n",
    "lgbm_acc = accuracy_score(y_val, lgbm_pred)\n",
    "print(f\"LightGBM (GPU): {lgbm_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c7cc528c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "MODEL COMPARISON (Accuracy)\n",
      "========================================\n",
      "              Model  Accuracy\n",
      "           LightGBM  0.679407\n",
      "            XGBoost  0.678307\n",
      "  Gradient Boosting  0.676964\n",
      "      Random Forest  0.664950\n",
      "Logistic Regression  0.664357\n",
      "========================================\n",
      "Best: LightGBM (0.6794)\n"
     ]
    }
   ],
   "source": [
    "# Results comparison\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'Gradient Boosting', 'XGBoost', 'LightGBM'],\n",
    "    'Accuracy': [lr_acc, rf_acc, gb_acc, xgb_acc, lgbm_acc]\n",
    "}).sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(\"MODEL COMPARISON (Accuracy)\")\n",
    "print(\"=\" * 40)\n",
    "print(results.to_string(index=False))\n",
    "print(\"=\" * 40)\n",
    "best_model = results.iloc[0]['Model']\n",
    "best_acc = results.iloc[0]['Accuracy']\n",
    "print(f\"Best: {best_model} ({best_acc:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978a23ea",
   "metadata": {},
   "source": [
    "We are going to continue with LightGBM and XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681875d8",
   "metadata": {},
   "source": [
    "# Model Improvement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "48098141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1f68ec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 5. Best value: 0.683361: 100%|██████████| 30/30 [08:20<00:00, 16.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost accuracy (CV): 0.6834\n",
      "CPU times: total: 14min 29s\n",
      "Wall time: 8min 20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Hyperparameter tuning for XGBoost with Optuna (GPU)\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'random_state': 42,\n",
    "        'tree_method': 'hist',\n",
    "        'device': 'cuda',\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    model = XGBClassifier(**params)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy')\n",
    "    return scores.mean()\n",
    "\n",
    "study_xgb = optuna.create_study(direction='maximize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "print(f\"Best XGBoost accuracy (CV): {study_xgb.best_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e37e9ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost params:\n",
      "  n_estimators: 466\n",
      "  max_depth: 5\n",
      "  learning_rate: 0.13685396236512357\n",
      "  min_child_weight: 9\n",
      "  subsample: 0.8167677754456445\n",
      "  colsample_bytree: 0.6056940961298176\n",
      "  reg_alpha: 1.5277483435281087e-06\n",
      "  reg_lambda: 1.1879967032598484\n"
     ]
    }
   ],
   "source": [
    "# Best XGBoost parameters found\n",
    "print(\"Best XGBoost params:\")\n",
    "for k, v in study_xgb.best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5f275bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized XGBoost (GPU): 0.6833 (before: 0.6783)\n",
      "Improvement: 0.50%\n",
      "CPU times: total: 10.6 s\n",
      "Wall time: 5.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train optimized XGBoost (GPU)\n",
    "xgb_opt = XGBClassifier(**study_xgb.best_params, random_state=42, tree_method='hist', device='cuda', verbosity=0)\n",
    "xgb_opt.fit(X_train, y_train)\n",
    "xgb_opt_pred = xgb_opt.predict(X_val)\n",
    "xgb_opt_acc = accuracy_score(y_val, xgb_opt_pred)\n",
    "\n",
    "print(f\"Optimized XGBoost (GPU): {xgb_opt_acc:.4f} (before: {xgb_acc:.4f})\")\n",
    "print(f\"Improvement: {(xgb_opt_acc - xgb_acc)*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e53c6dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 28. Best value: 0.683673: 100%|██████████| 30/30 [14:40<00:00, 29.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LightGBM accuracy (CV): 0.6837\n",
      "CPU times: total: 1h 22min 46s\n",
      "Wall time: 14min 40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Hyperparameter tuning for LightGBM with Optuna (GPU)\n",
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'random_state': 42,\n",
    "        'device': 'gpu',\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    model = LGBMClassifier(**params)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy')\n",
    "    return scores.mean()\n",
    "\n",
    "study_lgbm = optuna.create_study(direction='maximize')\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "print(f\"Best LightGBM accuracy (CV): {study_lgbm.best_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b7a56911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LightGBM params:\n",
      "  n_estimators: 352\n",
      "  max_depth: 11\n",
      "  learning_rate: 0.06501279852289088\n",
      "  num_leaves: 122\n",
      "  min_child_samples: 43\n",
      "  subsample: 0.7435239638223387\n",
      "  colsample_bytree: 0.6009309753449563\n",
      "  reg_alpha: 0.048005549238116146\n",
      "  reg_lambda: 0.781592542334054\n"
     ]
    }
   ],
   "source": [
    "# Best LightGBM parameters found\n",
    "print(\"Best LightGBM params:\")\n",
    "for k, v in study_lgbm.best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ec95cbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized LightGBM (GPU): 0.6849 (before: 0.6794)\n",
      "Improvement: 0.55%\n",
      "CPU times: total: 1min 36s\n",
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train optimized LightGBM (GPU)\n",
    "lgbm_opt = LGBMClassifier(**study_lgbm.best_params, random_state=42, device='gpu', verbose=-1)\n",
    "lgbm_opt.fit(X_train, y_train)\n",
    "lgbm_opt_pred = lgbm_opt.predict(X_val)\n",
    "lgbm_opt_acc = accuracy_score(y_val, lgbm_opt_pred)\n",
    "\n",
    "print(f\"Optimized LightGBM (GPU): {lgbm_opt_acc:.4f} (before: {lgbm_acc:.4f})\")\n",
    "print(f\"Improvement: {(lgbm_opt_acc - lgbm_acc)*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "df40aa65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL COMPARISON\n",
      "           Model  Accuracy\n",
      "LightGBM (tuned)  0.684886\n",
      " XGBoost (tuned)  0.683336\n",
      " LightGBM (base)  0.679407\n",
      "  XGBoost (base)  0.678307\n",
      "\n",
      "Best model: LightGBM (tuned) (0.6849)\n"
     ]
    }
   ],
   "source": [
    "# Final comparison\n",
    "final_results = pd.DataFrame({\n",
    "    'Model': ['LightGBM (base)', 'LightGBM (tuned)', 'XGBoost (base)', 'XGBoost (tuned)'],\n",
    "    'Accuracy': [lgbm_acc, lgbm_opt_acc, xgb_acc, xgb_opt_acc]\n",
    "}).sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"FINAL COMPARISON\")\n",
    "\n",
    "print(final_results.to_string(index=False))\n",
    "\n",
    "# Select best model\n",
    "if lgbm_opt_acc >= xgb_opt_acc:\n",
    "    best_final_model = lgbm_opt\n",
    "    best_final_name = \"LightGBM (tuned)\"\n",
    "    best_final_acc = lgbm_opt_acc\n",
    "else:\n",
    "    best_final_model = xgb_opt\n",
    "    best_final_name = \"XGBoost (tuned)\"\n",
    "    best_final_acc = xgb_opt_acc\n",
    "\n",
    "print(f\"\\nBest model: {best_final_name} ({best_final_acc:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a4ed6",
   "metadata": {},
   "source": [
    "# Kaggle Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca3165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model (GPU) trained on 700,000 samples\n"
     ]
    }
   ],
   "source": [
    "# Retrain best model on full training data (GPU)\n",
    "X_full = df_train[features_tree]\n",
    "y_full = df_train['diagnosed_diabetes']\n",
    "\n",
    "final_model = LGBMClassifier(**study_lgbm.best_params, random_state=42, device='gpu', verbose=-1)\n",
    "final_model.fit(X_full, y_full)\n",
    "\n",
    "print(f\"Final model trained on {len(X_full):,} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bdaa8753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions made for 300,000 test samples\n",
      "Prediction distribution: {1.0: 0.689, 0.0: 0.311}\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data and make predictions\n",
    "X_test = df_test[features_tree]\n",
    "test_predictions = final_model.predict(X_test)\n",
    "\n",
    "print(f\"Predictions made for {len(test_predictions):,} test samples\")\n",
    "print(f\"Prediction distribution: {pd.Series(test_predictions).value_counts(normalize=True).round(3).to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ebd6379a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to 'submission.csv'\n",
      "Shape: (300000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosed_diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>700000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>700001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>700002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>700003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>700004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>700005</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>700006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>700007</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>700008</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>700009</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  diagnosed_diabetes\n",
       "0  700000                   1\n",
       "1  700001                   1\n",
       "2  700002                   1\n",
       "3  700003                   0\n",
       "4  700004                   1\n",
       "5  700005                   1\n",
       "6  700006                   1\n",
       "7  700007                   1\n",
       "8  700008                   1\n",
       "9  700009                   1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': df_test['id'],\n",
    "    'diagnosed_diabetes': test_predictions.astype(int)\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"Submission saved to 'submission.csv'\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "submission.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
